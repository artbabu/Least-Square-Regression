{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'print', 'invalid': 'print', 'over': 'print', 'under': 'print'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.special as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations, permutations\n",
    "from numpy.linalg import inv\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import keras\n",
    "\n",
    "from keras.utils import np_utils\n",
    "import keras.callbacks as cb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "np.seterr(all='print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_feature_List = ['country_group_CAN','country_group_EURO', 'country_group_USA', 'Position_C',\n",
    "       'Position_D', 'Position_L', 'Position_R'] \n",
    "def standardize(s_df):\n",
    "    for col in s_df.columns.values:\n",
    "        col_mean = s_df[col].mean()\n",
    "        col_std = s_df[col].std()\n",
    "\n",
    "        #         mean of interaction terms of two discrete variable are zero. those columns are\n",
    "        #         filtered while standizing as it cause singular matrix\n",
    "        if (col_mean != 0 or col_std != 0) or col not in dummy_feature_List :\n",
    "            s_df[col] = s_df[col].apply(lambda x: (x - col_mean) / float(col_std))\n",
    "\n",
    "    return s_df\n",
    "\n",
    "\n",
    "def preprocessing(x_df):\n",
    "    x_df = pd.get_dummies(x_df, prefix=['country_group', 'Position'], columns=['country_group', 'Position'])\n",
    "    x_df = x_df.apply(pd.to_numeric, args=('coerce',))\n",
    "\n",
    "#     # # adding interaction terms\n",
    "#     for col_1, col_2 in combinations(x_df.columns, 2):\n",
    "#         cond1 = col_1 not in dummy_feature_List\n",
    "#         cond2 = col_2 not in dummy_feature_List\n",
    "#         if cond1 or cond2:\n",
    "#             x_df['{}*{}'.format(col_1, col_2)] = np.multiply(x_df[col_1], x_df[col_2])\n",
    "#     # x_df.to_csv('ex.csv', sep='\\t')\n",
    "\n",
    "# #     standardization\n",
    "#     x_df = standardize(x_df)\n",
    "# #     y_df = standardize(y_df)\n",
    "\n",
    "    #     deleteing column = 0\n",
    "    x_df = x_df.loc[:, (x_df != 0).any(axis=0)]\n",
    "#     x_df.insert(loc=0, column='x0', value=1)\n",
    "\n",
    "    return x_df\n",
    "\n",
    "def calculate_negative_log_likelihood(target, weights, features):\n",
    "    \n",
    "    z = np.dot(weights,features)\n",
    "    y = sps.expit(z)\n",
    "    mask = np.isinf(y)\n",
    "    y[mask] = -z[mask]\n",
    "    \n",
    "    likelihood = np.mean(np.sum((1-target)*z + np.log(y)))\n",
    "    \n",
    "    return likelihood\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    input = os.path.join(\"../MLAssign2/Least-Square-Regression/\",'Model_Trees_Full_Dataset', 'preprocessed_datasets.csv')\n",
    "    data = pd.read_csv(input)\n",
    "\n",
    "    # random shuffle\n",
    "    data = data.iloc[np.random.permutation(len(data))]\n",
    "    # data[u'GP_greater_than_0'] = data[u'GP_greater_than_0'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "    training_df = data[data[u'DraftYear'].isin([2004, 2005, 2006])]\n",
    "    testing_df = data[data[u'DraftYear'] == 2007]\n",
    "\n",
    "    drop_class = [u'id', u'Country', u'Overall', u'PlayerName', u'sum_7yr_TOI', u'DraftYear',\"GP_greater_than_0\"]\n",
    "    training_df.drop(drop_class, inplace=True, axis=1)\n",
    "    testing_df.drop(drop_class, inplace=True, axis=1)\n",
    "\n",
    "    y_train_df = training_df.filter([u'sum_7yr_GP'])\n",
    "    x_train_df = training_df.drop([u'sum_7yr_GP'], axis=1)\n",
    "\n",
    "    y_test_df = testing_df.filter([u'sum_7yr_GP'])\n",
    "    x_test_df = testing_df.drop([u'sum_7yr_GP'], axis=1)\n",
    "\n",
    "    x_train_df_processed = preprocessing(x_train_df)\n",
    "    x_test_df_processed = preprocessing(x_test_df)\n",
    "\n",
    "    y_train = y_train_df.values\n",
    "    x_train = x_train_df_processed.values\n",
    "    y_test = y_test_df.values\n",
    "    x_test = x_test_df_processed.values\n",
    "    \n",
    "    return (x_train,y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/aarthy/anaconda3/envs/NLP/lib/python2.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tol = 0.00001\n",
    "\n",
    "# Step size for gradient descent.\n",
    "etas = [0.5, 0.3, 0.1, 0.05, 0.01]\n",
    "# etas = [0.1, 0.05, 0.01]\n",
    "\n",
    "(x_train,y_train), (x_test, y_test) = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18,  74, 170, ...,   1,   0,   0],\n",
       "       [ 19,  72, 194, ...,   1,   0,   0],\n",
       "       [ 18,  73, 183, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [ 19,  80, 249, ...,   0,   1,   0],\n",
       "       [ 18,  72, 203, ...,   0,   0,   0],\n",
       "       [ 20,  74, 220, ...,   1,   0,   0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 22)\n",
      "(637, 1)\n",
      "(191, 22)\n",
      "(191, 1)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compield in 0.101611852646 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = Sequential()\n",
    "model.add(Dense(units=50, input_dim=22, activation='relu' ))\n",
    "model.add(Dense(units=1000, activation='relu' ))\n",
    "model.add(Dense(1,activation='linear',use_bias=True))\n",
    "# model.compile(loss=\"mean_squared_error\", optimizer ='adam', metrics=['accuracy'])\n",
    "\n",
    "# rms = RMSprop()\n",
    "sgd = keras.optimizers.SGD(lr=1)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd, metrics=['accuracy'])\n",
    "print 'Model compield in {0} seconds'.format(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 211 samples\n",
      "Epoch 1/150\n",
      "426/426 [==============================] - 0s - loss: 551995691532891712.0000 - acc: 0.1831 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 2/150\n",
      "426/426 [==============================] - 0s - loss: 60.5258 - acc: 0.1995 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 3/150\n",
      "426/426 [==============================] - 0s - loss: 60.5681 - acc: 0.1995 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 4/150\n",
      "426/426 [==============================] - 0s - loss: 60.6362 - acc: 0.1808 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 5/150\n",
      "426/426 [==============================] - 0s - loss: 60.6056 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 6/150\n",
      "426/426 [==============================] - 0s - loss: 60.5751 - acc: 0.1972 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 7/150\n",
      "426/426 [==============================] - 0s - loss: 60.6268 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 8/150\n",
      "426/426 [==============================] - 0s - loss: 60.5845 - acc: 0.1831 - val_loss: 73.4455 - val_acc: 0.0190\n",
      "Epoch 9/150\n",
      "426/426 [==============================] - 0s - loss: 60.6549 - acc: 0.1808 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 10/150\n",
      "426/426 [==============================] - 0s - loss: 60.4765 - acc: 0.2207 - val_loss: 74.1517 - val_acc: 0.0000e+00\n",
      "Epoch 11/150\n",
      "426/426 [==============================] - 0s - loss: 60.7300 - acc: 0.1620 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 12/150\n",
      "426/426 [==============================] - 0s - loss: 60.6291 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 13/150\n",
      "426/426 [==============================] - 0s - loss: 60.5235 - acc: 0.1925 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 14/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1831 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 15/150\n",
      "426/426 [==============================] - 0s - loss: 60.5869 - acc: 0.1784 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 16/150\n",
      "426/426 [==============================] - 0s - loss: 60.6408 - acc: 0.1761 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 17/150\n",
      "426/426 [==============================] - 0s - loss: 60.5704 - acc: 0.1972 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 18/150\n",
      "426/426 [==============================] - 0s - loss: 60.6080 - acc: 0.1784 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 19/150\n",
      "426/426 [==============================] - 0s - loss: 60.6291 - acc: 0.1784 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 20/150\n",
      "426/426 [==============================] - 0s - loss: 60.5728 - acc: 0.1995 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 21/150\n",
      "426/426 [==============================] - 0s - loss: 60.5892 - acc: 0.1831 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 22/150\n",
      "426/426 [==============================] - 0s - loss: 60.6009 - acc: 0.1808 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 23/150\n",
      "426/426 [==============================] - 0s - loss: 60.5469 - acc: 0.2019 - val_loss: 73.2701 - val_acc: 0.0000e+00\n",
      "Epoch 24/150\n",
      "426/426 [==============================] - 0s - loss: 60.5892 - acc: 0.1878 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 25/150\n",
      "426/426 [==============================] - 0s - loss: 60.5939 - acc: 0.1854 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 26/150\n",
      "426/426 [==============================] - 0s - loss: 60.5775 - acc: 0.1925 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 27/150\n",
      "426/426 [==============================] - 0s - loss: 60.6385 - acc: 0.1690 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 28/150\n",
      "426/426 [==============================] - 0s - loss: 60.5915 - acc: 0.1831 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 29/150\n",
      "426/426 [==============================] - 0s - loss: 60.5704 - acc: 0.1995 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 30/150\n",
      "426/426 [==============================] - 0s - loss: 60.6408 - acc: 0.1784 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 31/150\n",
      "426/426 [==============================] - 0s - loss: 60.5610 - acc: 0.1972 - val_loss: 73.6588 - val_acc: 0.0142\n",
      "Epoch 32/150\n",
      "426/426 [==============================] - 0s - loss: 60.6737 - acc: 0.1714 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 33/150\n",
      "426/426 [==============================] - 0s - loss: 60.6197 - acc: 0.1831 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 34/150\n",
      "426/426 [==============================] - 0s - loss: 60.6362 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 35/150\n",
      "426/426 [==============================] - 0s - loss: 60.6150 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 36/150\n",
      "426/426 [==============================] - 0s - loss: 60.6526 - acc: 0.1643 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 37/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1831 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 38/150\n",
      "426/426 [==============================] - 0s - loss: 60.6033 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 39/150\n",
      "426/426 [==============================] - 0s - loss: 60.6385 - acc: 0.1831 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 40/150\n",
      "426/426 [==============================] - 0s - loss: 60.5493 - acc: 0.2042 - val_loss: 73.2701 - val_acc: 0.0000e+00\n",
      "Epoch 41/150\n",
      "426/426 [==============================] - 0s - loss: 60.6221 - acc: 0.1737 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 42/150\n",
      "426/426 [==============================] - 0s - loss: 60.5610 - acc: 0.1925 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 43/150\n",
      "426/426 [==============================] - 0s - loss: 60.5610 - acc: 0.1925 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 44/150\n",
      "426/426 [==============================] - 0s - loss: 60.5915 - acc: 0.1925 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 45/150\n",
      "426/426 [==============================] - 0s - loss: 60.6009 - acc: 0.1854 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 46/150\n",
      "426/426 [==============================] - 0s - loss: 60.6408 - acc: 0.1714 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 47/150\n",
      "426/426 [==============================] - 0s - loss: 60.6385 - acc: 0.1761 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 48/150\n",
      "426/426 [==============================] - 0s - loss: 60.5587 - acc: 0.1925 - val_loss: 73.4455 - val_acc: 0.0190\n",
      "Epoch 49/150\n",
      "426/426 [==============================] - 0s - loss: 60.6197 - acc: 0.1925 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 50/150\n",
      "426/426 [==============================] - 0s - loss: 60.6197 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 51/150\n",
      "426/426 [==============================] - 0s - loss: 60.5634 - acc: 0.2019 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 52/150\n",
      "426/426 [==============================] - 0s - loss: 60.6150 - acc: 0.1808 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 53/150\n",
      "426/426 [==============================] - 0s - loss: 60.6127 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 54/150\n",
      "426/426 [==============================] - 0s - loss: 60.5610 - acc: 0.1901 - val_loss: 73.2701 - val_acc: 0.0000e+00\n",
      "Epoch 55/150\n",
      "426/426 [==============================] - 0s - loss: 60.6315 - acc: 0.1737 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 56/150\n",
      "426/426 [==============================] - 0s - loss: 60.6244 - acc: 0.1878 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 57/150\n",
      "426/426 [==============================] - 0s - loss: 60.5962 - acc: 0.1901 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 58/150\n",
      "426/426 [==============================] - 0s - loss: 60.6221 - acc: 0.1878 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 59/150\n",
      "426/426 [==============================] - 0s - loss: 60.6221 - acc: 0.1784 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 60/150\n",
      "426/426 [==============================] - 0s - loss: 60.5681 - acc: 0.1878 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 61/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1878 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 62/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1737 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 63/150\n",
      "426/426 [==============================] - 0s - loss: 60.5915 - acc: 0.1901 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 64/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s - loss: 60.5986 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 65/150\n",
      "426/426 [==============================] - 0s - loss: 60.6291 - acc: 0.1831 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 66/150\n",
      "426/426 [==============================] - 0s - loss: 60.5775 - acc: 0.1925 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 67/150\n",
      "426/426 [==============================] - 0s - loss: 60.6385 - acc: 0.1690 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 68/150\n",
      "426/426 [==============================] - 0s - loss: 60.5751 - acc: 0.1878 - val_loss: 73.4455 - val_acc: 0.0190\n",
      "Epoch 69/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1784 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 70/150\n",
      "426/426 [==============================] - 0s - loss: 60.6291 - acc: 0.1761 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 71/150\n",
      "426/426 [==============================] - 0s - loss: 60.5892 - acc: 0.1901 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 72/150\n",
      "426/426 [==============================] - 0s - loss: 60.6338 - acc: 0.1737 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 73/150\n",
      "426/426 [==============================] - 1s - loss: 60.6479 - acc: 0.1690 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 74/150\n",
      "426/426 [==============================] - 0s - loss: 60.6127 - acc: 0.1878 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 75/150\n",
      "426/426 [==============================] - 0s - loss: 60.6338 - acc: 0.1737 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 76/150\n",
      "426/426 [==============================] - 0s - loss: 60.6620 - acc: 0.1596 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 77/150\n",
      "426/426 [==============================] - 0s - loss: 60.6033 - acc: 0.1784 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 78/150\n",
      "426/426 [==============================] - 0s - loss: 60.5892 - acc: 0.1948 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 79/150\n",
      "426/426 [==============================] - 0s - loss: 60.6103 - acc: 0.1831 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 80/150\n",
      "426/426 [==============================] - 0s - loss: 60.6268 - acc: 0.1737 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 81/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 82/150\n",
      "426/426 [==============================] - 0s - loss: 60.6268 - acc: 0.1808 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 83/150\n",
      "426/426 [==============================] - 0s - loss: 60.6338 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 84/150\n",
      "426/426 [==============================] - 0s - loss: 60.6667 - acc: 0.1643 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 85/150\n",
      "426/426 [==============================] - 0s - loss: 60.5681 - acc: 0.1995 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 86/150\n",
      "426/426 [==============================] - 0s - loss: 60.5704 - acc: 0.1878 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 87/150\n",
      "426/426 [==============================] - 0s - loss: 60.6268 - acc: 0.1761 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 88/150\n",
      "426/426 [==============================] - 0s - loss: 60.5962 - acc: 0.1972 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 89/150\n",
      "426/426 [==============================] - 0s - loss: 60.5869 - acc: 0.1972 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 90/150\n",
      "426/426 [==============================] - 0s - loss: 60.6291 - acc: 0.1737 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 91/150\n",
      "426/426 [==============================] - 0s - loss: 60.6127 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 92/150\n",
      "426/426 [==============================] - 0s - loss: 60.5751 - acc: 0.1808 - val_loss: 73.6588 - val_acc: 0.0142\n",
      "Epoch 93/150\n",
      "426/426 [==============================] - 0s - loss: 60.6362 - acc: 0.1761 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 94/150\n",
      "426/426 [==============================] - 0s - loss: 60.5141 - acc: 0.2066 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 95/150\n",
      "426/426 [==============================] - 0s - loss: 60.6432 - acc: 0.1761 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 96/150\n",
      "426/426 [==============================] - 0s - loss: 60.6150 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 97/150\n",
      "426/426 [==============================] - 0s - loss: 60.6127 - acc: 0.1831 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 98/150\n",
      "426/426 [==============================] - 0s - loss: 60.6080 - acc: 0.1854 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 99/150\n",
      "426/426 [==============================] - 0s - loss: 60.6033 - acc: 0.1784 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 100/150\n",
      "426/426 [==============================] - 0s - loss: 60.5869 - acc: 0.1948 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 101/150\n",
      "426/426 [==============================] - 0s - loss: 60.6784 - acc: 0.1549 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 102/150\n",
      "426/426 [==============================] - 0s - loss: 60.6174 - acc: 0.1737 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 103/150\n",
      "426/426 [==============================] - 0s - loss: 60.6667 - acc: 0.1714 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 104/150\n",
      "426/426 [==============================] - 0s - loss: 60.6432 - acc: 0.1714 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 105/150\n",
      "426/426 [==============================] - 0s - loss: 60.6197 - acc: 0.1831 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 106/150\n",
      "426/426 [==============================] - 0s - loss: 60.6408 - acc: 0.1690 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 107/150\n",
      "426/426 [==============================] - 0s - loss: 60.6268 - acc: 0.1831 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 108/150\n",
      "426/426 [==============================] - 0s - loss: 60.5939 - acc: 0.1831 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 109/150\n",
      "426/426 [==============================] - 0s - loss: 60.6056 - acc: 0.1854 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 110/150\n",
      "426/426 [==============================] - 0s - loss: 60.6150 - acc: 0.1831 - val_loss: 73.2701 - val_acc: 0.0000e+00\n",
      "Epoch 111/150\n",
      "426/426 [==============================] - 0s - loss: 60.5939 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 112/150\n",
      "426/426 [==============================] - 0s - loss: 60.5775 - acc: 0.1948 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 113/150\n",
      "426/426 [==============================] - 0s - loss: 60.5939 - acc: 0.1737 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 114/150\n",
      "426/426 [==============================] - 0s - loss: 60.5376 - acc: 0.1878 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 115/150\n",
      "426/426 [==============================] - 0s - loss: 60.6080 - acc: 0.1854 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 116/150\n",
      "426/426 [==============================] - 0s - loss: 60.5915 - acc: 0.1925 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 117/150\n",
      "426/426 [==============================] - 0s - loss: 60.6103 - acc: 0.1808 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 118/150\n",
      "426/426 [==============================] - 0s - loss: 60.6502 - acc: 0.1761 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 119/150\n",
      "426/426 [==============================] - 0s - loss: 60.6362 - acc: 0.1784 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 120/150\n",
      "426/426 [==============================] - 0s - loss: 60.5892 - acc: 0.1925 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 121/150\n",
      "426/426 [==============================] - 0s - loss: 60.6338 - acc: 0.1737 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 122/150\n",
      "426/426 [==============================] - 0s - loss: 60.5610 - acc: 0.1878 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 123/150\n",
      "426/426 [==============================] - 0s - loss: 60.6432 - acc: 0.1737 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 124/150\n",
      "426/426 [==============================] - 0s - loss: 60.5587 - acc: 0.1854 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 125/150\n",
      "426/426 [==============================] - 0s - loss: 60.5962 - acc: 0.1854 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 126/150\n",
      "426/426 [==============================] - 0s - loss: 60.5986 - acc: 0.1948 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 127/150\n",
      "426/426 [==============================] - 0s - loss: 60.6033 - acc: 0.1808 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 128/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s - loss: 60.5446 - acc: 0.2066 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 129/150\n",
      "426/426 [==============================] - 0s - loss: 60.6033 - acc: 0.1784 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 130/150\n",
      "426/426 [==============================] - 0s - loss: 60.5845 - acc: 0.1878 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 131/150\n",
      "426/426 [==============================] - 0s - loss: 60.6268 - acc: 0.1878 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 132/150\n",
      "426/426 [==============================] - 0s - loss: 60.5352 - acc: 0.2019 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 133/150\n",
      "426/426 [==============================] - 0s - loss: 60.4765 - acc: 0.1995 - val_loss: 74.6635 - val_acc: 0.0047\n",
      "Epoch 134/150\n",
      "426/426 [==============================] - 0s - loss: 60.7207 - acc: 0.1737 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 135/150\n",
      "426/426 [==============================] - 0s - loss: 60.6127 - acc: 0.1854 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 136/150\n",
      "426/426 [==============================] - 0s - loss: 60.5892 - acc: 0.1878 - val_loss: 73.0948 - val_acc: 0.0047\n",
      "Epoch 137/150\n",
      "426/426 [==============================] - 0s - loss: 60.5516 - acc: 0.1901 - val_loss: 73.6588 - val_acc: 0.0142\n",
      "Epoch 138/150\n",
      "426/426 [==============================] - 0s - loss: 60.6831 - acc: 0.1690 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 139/150\n",
      "426/426 [==============================] - 0s - loss: 60.6526 - acc: 0.1643 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 140/150\n",
      "426/426 [==============================] - 0s - loss: 60.5634 - acc: 0.1972 - val_loss: 73.2701 - val_acc: 0.0000e+00\n",
      "Epoch 141/150\n",
      "426/426 [==============================] - 0s - loss: 60.6667 - acc: 0.1690 - val_loss: 72.9289 - val_acc: 0.0142\n",
      "Epoch 142/150\n",
      "426/426 [==============================] - 0s - loss: 60.5822 - acc: 0.1972 - val_loss: 73.2701 - val_acc: 0.0000e+00\n",
      "Epoch 143/150\n",
      "426/426 [==============================] - 0s - loss: 60.6056 - acc: 0.1854 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 144/150\n",
      "426/426 [==============================] - 0s - loss: 60.6315 - acc: 0.1737 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 145/150\n",
      "426/426 [==============================] - 0s - loss: 60.5258 - acc: 0.1972 - val_loss: 72.6303 - val_acc: 0.5308\n",
      "Epoch 146/150\n",
      "426/426 [==============================] - 0s - loss: 60.6197 - acc: 0.1784 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 147/150\n",
      "426/426 [==============================] - 0s - loss: 60.6479 - acc: 0.1620 - val_loss: 72.6919 - val_acc: 0.0190\n",
      "Epoch 148/150\n",
      "426/426 [==============================] - 0s - loss: 60.6479 - acc: 0.1620 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 149/150\n",
      "426/426 [==============================] - 0s - loss: 60.6080 - acc: 0.1784 - val_loss: 72.7915 - val_acc: 0.0190\n",
      "Epoch 150/150\n",
      "426/426 [==============================] - 0s - loss: 60.5775 - acc: 0.1878 - val_loss: 73.0948 - val_acc: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f011bd94cd0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=150, batch_size=1)\n",
    "# model.fit(x_test, y_test, epochs=150,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating on testing set...\n",
      "128/191 [===================>..........] - ETA: 0s[INFO] loss=64.0105, accuracy: 1.5707%\n"
     ]
    }
   ],
   "source": [
    "# show the accuracy on the testing set\n",
    "print(\"[INFO] evaluating on testing set...\")\n",
    "(loss, accuracy) = model.evaluate(x_test, y_test,\n",
    "\tbatch_size=128, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "\taccuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9]\n",
      " [  0]\n",
      " [203]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [415]\n",
      " [136]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [515]\n",
      " [324]\n",
      " [189]\n",
      " [  1]\n",
      " [  0]\n",
      " [282]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 35]\n",
      " [  0]\n",
      " [108]\n",
      " [  0]\n",
      " [  1]\n",
      " [ 18]\n",
      " [ 25]\n",
      " [  0]\n",
      " [203]\n",
      " [  0]\n",
      " [124]\n",
      " [  0]\n",
      " [ 87]\n",
      " [297]\n",
      " [ 11]\n",
      " [  0]\n",
      " [ 25]\n",
      " [136]\n",
      " [  0]\n",
      " [  0]\n",
      " [246]\n",
      " [  3]\n",
      " [ 79]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [449]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 58]\n",
      " [  0]\n",
      " [ 10]\n",
      " [ 13]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 87]\n",
      " [132]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [176]\n",
      " [ 29]\n",
      " [  0]\n",
      " [ 63]\n",
      " [ 75]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [119]\n",
      " [  0]\n",
      " [  0]\n",
      " [157]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 81]\n",
      " [283]\n",
      " [ 67]\n",
      " [ 61]\n",
      " [154]\n",
      " [  0]\n",
      " [481]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  7]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 72]\n",
      " [102]\n",
      " [110]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  1]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  5]\n",
      " [316]\n",
      " [  0]\n",
      " [  2]\n",
      " [ 54]\n",
      " [  0]\n",
      " [297]\n",
      " [217]\n",
      " [164]\n",
      " [284]\n",
      " [246]\n",
      " [ 28]\n",
      " [344]\n",
      " [ 40]\n",
      " [  0]\n",
      " [319]\n",
      " [286]\n",
      " [ 72]\n",
      " [ 16]\n",
      " [113]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 55]\n",
      " [  0]\n",
      " [106]\n",
      " [  0]\n",
      " [132]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 25]\n",
      " [  0]\n",
      " [  0]\n",
      " [304]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 10]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 68]\n",
      " [345]\n",
      " [ 12]\n",
      " [  0]\n",
      " [ 22]\n",
      " [ 58]\n",
      " [  0]\n",
      " [120]\n",
      " [  0]\n",
      " [ 10]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [211]\n",
      " [  3]\n",
      " [  0]\n",
      " [418]\n",
      " [  0]\n",
      " [449]\n",
      " [  0]\n",
      " [ 22]\n",
      " [  0]\n",
      " [  0]\n",
      " [ 20]\n",
      " [  0]\n",
      " [ 77]\n",
      " [121]\n",
      " [126]\n",
      " [129]\n",
      " [184]\n",
      " [  0]\n",
      " [ 87]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  3]\n",
      " [  0]]\n"
     ]
    }
   ],
   "source": [
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
